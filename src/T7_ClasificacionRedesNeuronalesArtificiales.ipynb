{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">7. Clasificación</h1>\n",
    "<h2 style=\"text-align: center;\">Redes neuronales artificiales</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elaborado por el Dr. Carlos Francisco Méndez para la asignatura de Introducción a la Ciencia de Datos de la Licenciatura en Ciencias Genómicas\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Entrenar un modelo de red neuronal artificial profunda para resolver una tarea de clasificación multiclase usando PyTorch.\n",
    "\n",
    "Notebook adaptado de la [Capacitación oficial de PyTorch.](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "\n",
    "## Descripción de la tarea\n",
    "\n",
    "En este taller vamos a user el conjunto de datos FashionMNIST. El modelo debe clasificar imágenes de elementos de vestimenta en una de 10 posibles categorías:\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot        \n",
    "    \n",
    "Ejemplos de entrada:\n",
    "<img \n",
    "    style=\"display: block; \n",
    "           margin-left: auto;\n",
    "           margin-right: auto;\n",
    "           width: 50%;\"\n",
    "    src=\"fashion-mnist-sprite.png\" \n",
    "    alt=\"FashionMNIST\">\n",
    "</img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga y creación de conjuntos de entrenamiento y prueba\n",
    "\n",
    "Usaremos dos objetos de PyTorch para manejo de datos:\n",
    "* Dataset: permite almacenar conjuntos de datos en forma de `Ejemplo - Etiqueta`.\n",
    "* DataLoader: permite iterar a través del conjunto de datos.\n",
    "\n",
    "El módulo `torchvision.datasets` contiene objetos de la clase `Dataset` para acceder a diversos conjuntos de datos disponibles para uso libre. \n",
    "\n",
    "Usaremos el conjunto de datos `FashionMNIS`. El argumento `transform` permite transformar los datos en tensores con el parámetro ToTensor() y el argumento `download=True` indica que el dataset debe ser descargado.\n",
    "\n",
    "Dividimos el conjunto de datos en dos subconjuntos. Unos será usado para el entrenamiento de la red (`train_data`) y el otro para evaluarla (`test_data`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos\n",
      "    age_year  cholesterol  gluc  smoke  alco        bmi\n",
      "0  50.391781            1     1      0     0  21.967120\n",
      "1  55.419178            3     1      0     0  34.927679\n",
      "2  51.663014            3     1      0     0  23.507805\n",
      "3  48.282192            1     1      0     0  28.710479\n",
      "4  47.873973            1     1      0     0  23.011177\n",
      "Categorias de clase\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: cardio, dtype: int64\n",
      "Datos\n",
      "[[-0.41252876 -0.51957914 -0.38479629 -0.30994665 -0.23537015 -1.12322809]\n",
      " [ 0.33066899  2.48212624 -0.38479629 -0.30994665 -0.23537015  1.82539772]\n",
      " [-0.22460301  2.48212624 -0.38479629 -0.30994665 -0.23537015 -0.77271049]\n",
      " ...\n",
      " [-0.06948326 -0.51957914 -0.38479629  3.22636167 -0.23537015  0.00528188]\n",
      " [ 1.2229113  -0.51957914  1.38952461 -0.30994665 -0.23537015  0.04437073]\n",
      " [ 0.45703285  0.98127355 -0.38479629 -0.30994665 -0.23537015 -0.45290544]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/cvd_transformed.csv\")\n",
    "data.head(6)\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, np.r_[2, 8:12, 14:15]]\n",
    "y = data.iloc[:, 13]  # all rows, label only\n",
    "print(\"Datos\")\n",
    "print(X.head())\n",
    "print(\"Categorias de clase\")\n",
    "print(y.head())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "print(\"Datos\")\n",
    "print(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ensure X_train and y_train are numpy arrays\n",
    "X_train = X_train.to_numpy() if hasattr(X_train, 'to_numpy') else X_train\n",
    "y_train = y_train.to_numpy() if hasattr(y_train, 'to_numpy') else y_train\n",
    "\n",
    "X_test = X_test.to_numpy() if hasattr(X_test, 'to_numpy') else X_test\n",
    "y_test = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else y_test\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use long for classification labels\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de `DataLoader`\n",
    "\n",
    "Definimos un tamaño de lote para que la red ajuste sus pesos cada 64 ejemplos (`batch_size = 64`).\n",
    "\n",
    "Creamos dos objetos `DataLoader` para iterar ambos conjuntos de datos por lotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observación del DataLoader\n",
    "\n",
    "Vamos a iterar sobre el DataLoader de prueba (`test_dataloader`) para ver observar la dimesionalidad del primer lote y su categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad de datos X [TamañoLote, CanalImag, LargoImag, AnchoImag]: torch.Size([64, 6])\n",
      "Dimensionalidad de categorías y: torch.Size([64]), tipo de categorías: torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Dimensionalidad de datos X [TamañoLote, CanalImag, LargoImag, AnchoImag]: {X.shape}\")\n",
    "    print(f\"Dimensionalidad de categorías y: {y.shape}, tipo de categorías: {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la red neuronal\n",
    "\n",
    "### Definición de `device` para entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device a utulizar: cpu\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Device a utulizar: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de arquitectura de la red neuronal\n",
    "\n",
    "Creamos la clase `RedNeuronal` a partir de la clase base [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Esta clase será definida de acuerdo a la arquitectura de red que deseamos entrenar.\n",
    "\n",
    "Debemos definir:\n",
    "- Capas\n",
    "- Nodos por capa\n",
    "- Función de activación\n",
    "- Procesamiento hacia adelante de la red (`forward`)\n",
    "\n",
    "Al final indicamos que la red debe crearse en el `device` definido anteriormente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RedNeuronal(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define modelo\n",
    "class RedNeuronal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = RedNeuronal().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de función de pérdida y optimizador\n",
    "\n",
    "Para que la red neuronal aprenda debemos utilizar una [función de pérdida o loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) que mida las predicciones de la red y un [optimizador](https://pytorch.org/docs/stable/optim.html) que ajuste los pesos de la red neuronal para intenter predecir cada vez mejor, esto es, diminuir el valor de la función de pérdida.\n",
    "\n",
    "La función de pérdida más recomendada para resolver tareas de clasificación multiclase es la Entropía crusada (`CrossEntropyLoss`).\n",
    "\n",
    "El optimizador más sencillo es uno basado de el algortimo de Descenso por gradiente estocástico (`SGD`). La tasa de aprendizaje (learning rate o `lr`) será definida como 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de método de entrenamiento (`train`)\n",
    "\n",
    "Esto incluye procesar los datos del `dataloader` lote por lote \"hacia delante\" y calcular el error de predicción de la red, esto es, el valor de la función de pérdida. Luego debemos propagar el error de predicción \"hacia atrás\" de la red para ajustar los pesos de los nodos. En cada iteración, la red ajustará los pesos para predecir cada vez mejor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"Valor de función de pérdida: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de método de prueba (`test`)\n",
    "\n",
    "Esto incluye procesar los datos de prueba lote por lote, esto es, hacer que la red previamente entrenada realice predicciones sobre estos datos y acumular tanto el error (valor de la función de perdida) como los aciertos de la red (valor de exactitud o `Accuracy`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Fase de prueba: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red neuronal\n",
    "\n",
    "Ejecutamos los métodos de entrenamiento (`train`) y prueba (`test`) por un número de épocas (veces que la red entrena usando todos los datos disponibles).\n",
    "\n",
    "El `train` imprimirá el valor de pérdida por cada 100 lotes. El método `test` imprimirá el valro de pérdida y la exactitud de predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x6 and 784x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Compute prediction error\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mRedNeuronal.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m---> 16\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nahua\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x6 and 784x512)"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar modelo entrenado\n",
    "\n",
    "Para guardar nuestro modelo entrenado, almacenamos en un archivo los pesos ajustados finales de la red neuronal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardamos modelo entrenado en model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Guardamos modelo entrenado en model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase de inferencia\n",
    "\n",
    "\n",
    "### Cargar modelo entrenado\n",
    "\n",
    "Para usar nuestro modelo entrenado, creamos una instancia de la red neuronal y le cargamos los pesos almacenados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RedNeuronal().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar modelo entrenado\n",
    "\n",
    "Predecir con el modelo entrenado la categoría de nuevos datos. Como ejemplo, usemos la imagen 919 del conjunto de datos de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría: Trouser\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "image, label = test_data[5]\n",
    "pyplot.imshow(image[0], cmap='gray')\n",
    "print('Categoría:', classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos nuestro modelo entrenado en modo de evaluación `model.eval()` y sin ajustes de pesos: `torch.no_grad()`.\n",
    "\n",
    "Imprimimos la categoría predicha por el modelo y la categoría verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categoría predicha: \"Trouser\", Categoría verdadera: \"Trouser\"\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x, y = test_data[5][0], test_data[5][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Categoría predicha: \"{predicted}\", Categoría verdadera: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
